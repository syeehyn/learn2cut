{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:53:48.458618Z",
     "start_time": "2021-04-15T07:53:47.995487Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:56:04.327378Z",
     "start_time": "2021-04-15T07:56:04.310040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir ../instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 9\n"
     ]
    }
   ],
   "source": [
    "easy_config = {\n",
    "    \"load_dir\"        : '../instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 10,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "env = make_multiple_env(**easy_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T08:03:38.164560Z",
     "start_time": "2021-04-15T08:03:38.146665Z"
    }
   },
   "outputs": [],
   "source": [
    "class policyNet(nn.Module):\n",
    "    def __init__(self, size_dim, hidden_size, output_size):\n",
    "        super(policyNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(size_dim, size_dim)\n",
    "        self.fc1 = nn.Linear(size_dim-1, size_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(size_dim, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.hidden_combine = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.dropout = nn.Dropout(p = .2)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "#         self.fc1 = nn.Linear(size_dim, hidden)\n",
    "    def forward(self, s, hidden):\n",
    "        A, b, c, E, d = self._preproc(s)\n",
    "        Ab = torch.hstack((A, b.unsqueeze(1)))\n",
    "        Ed = torch.hstack((E, d.unsqueeze(1)))\n",
    "        \n",
    "        c = self.embedding(c)\n",
    "        \n",
    "        Ab = Ab @ c.T\n",
    "        \n",
    "        Ab = F.relu(self.fc1(Ab))\n",
    "        h, h_hidden = self.gru(Ab.unsqueeze(0), hidden.unsqueeze(0))\n",
    "        \n",
    "        g, g_hidden = self.gru(Ed.unsqueeze(0), hidden.unsqueeze(0))\n",
    "        \n",
    "        # h = self.dropout(h)\n",
    "        # g = self.dropout(g)\n",
    "\n",
    "        h = self.out(h.squeeze(0))\n",
    "        g = self.out(g.squeeze(0))\n",
    "        \n",
    "        hidden = torch.cat((h_hidden.squeeze(0), g_hidden.squeeze(0)), 1)\n",
    "        \n",
    "        hidden = self.hidden_combine(F.relu(hidden))\n",
    "        \n",
    "        S = torch.mean(h @ g.T, 0)\n",
    "        S = F.log_softmax(S, dim=-1)\n",
    "        \n",
    "        return S , hidden\n",
    "        \n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "        \n",
    "    def _preproc(self, s):\n",
    "        min1 = min(s[0].min(), s[-2].min())\n",
    "        max1 = max(s[0].max(), s[-2].max())\n",
    "        min2 = min(s[1].min(), s[-1].min())\n",
    "        max2 = max(s[1].max(), s[-1].max())\n",
    "\n",
    "        A = torch.FloatTensor((s[0] - min1) / (max1 - min1))\n",
    "        E = torch.FloatTensor((s[-2] - min1) / (max1 - min1))\n",
    "        b = torch.FloatTensor((s[1] - min2) / (max2 - min2))\n",
    "        d = torch.FloatTensor((s[-1] - min2) / (max2 - min2))\n",
    "        return [A, b, torch.LongTensor(s[2]), E, d]\n",
    "def discounted_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(0,len(r))):\n",
    "        discounted_r[i] = running_sum * gamma + r[i]\n",
    "        running_sum = discounted_r[i]\n",
    "    return torch.FloatTensor(discounted_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T08:04:49.337890Z",
     "start_time": "2021-04-15T08:04:35.410916Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir ../instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 9\n",
      "episode 0 step 0 reward 0.018784107402780137 action space size 61 action 31\n",
      "episode 0 step 1 reward 0.037863926595264275 action space size 63 action 20\n",
      "episode 0 step 2 reward 0.013700992192525518 action space size 63 action 38\n",
      "episode 0 step 3 reward 0.034609637235234914 action space size 64 action 35\n",
      "episode 0 step 4 reward 0.01414928674512339 action space size 62 action 54\n",
      "episode 0 step 5 reward 0.007497068687371211 action space size 66 action 43\n",
      "episode 0 step 6 reward 0.0031620267495782173 action space size 67 action 23\n",
      "episode 0 step 7 reward 0.000902974228210951 action space size 66 action 57\n",
      "episode 0 step 8 reward 0.0021965601854390115 action space size 69 action 9\n",
      "episode 0 step 9 reward 0.015988799718797964 action space size 69 action 53\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "loss: 2.0684172004461288\n",
      "episode 1 step 0 reward 0.7058461509213885 action space size 60 action 0\n",
      "episode 1 step 1 reward 0.0 action space size 60 action 38\n",
      "episode 1 step 2 reward 4.547473508864641e-13 action space size 61 action 0\n",
      "episode 1 step 3 reward 0.0 action space size 64 action 7\n",
      "episode 1 step 4 reward 4.547473508864641e-13 action space size 64 action 2\n",
      "episode 1 step 5 reward 0.0 action space size 63 action 32\n",
      "episode 1 step 6 reward 0.0 action space size 65 action 38\n",
      "episode 1 step 7 reward 4.547473508864641e-13 action space size 65 action 7\n",
      "episode 1 step 8 reward 4.547473508864641e-13 action space size 67 action 62\n",
      "episode 1 step 9 reward 0.0 action space size 67 action 50\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "loss: 26.32311700284481\n",
      "episode 2 step 0 reward 0.615175166921972 action space size 60 action 0\n",
      "episode 2 step 1 reward 1.1368683772161603e-12 action space size 58 action 29\n",
      "episode 2 step 2 reward 0.0 action space size 61 action 6\n",
      "episode 2 step 3 reward 2.2737367544323206e-13 action space size 62 action 58\n",
      "episode 2 step 4 reward 2.2737367544323206e-13 action space size 63 action 18\n",
      "episode 2 step 5 reward 2.2737367544323206e-13 action space size 65 action 13\n",
      "episode 2 step 6 reward 4.547473508864641e-13 action space size 66 action 0\n",
      "episode 2 step 7 reward 6.821210263296962e-13 action space size 66 action 39\n",
      "episode 2 step 8 reward 0.0 action space size 68 action 20\n",
      "episode 2 step 9 reward 2.2737367544323206e-13 action space size 67 action 59\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "loss: 65.5087091833353\n",
      "episode 3 step 0 reward 0.0032719085634198564 action space size 61 action 11\n",
      "episode 3 step 1 reward 0.9183441050040528 action space size 59 action 0\n",
      "episode 3 step 2 reward 2.2737367544323206e-13 action space size 63 action 1\n",
      "episode 3 step 3 reward 0.0 action space size 63 action 59\n",
      "episode 3 step 4 reward 0.0 action space size 64 action 47\n",
      "episode 3 step 5 reward 0.0 action space size 63 action 26\n",
      "episode 3 step 6 reward 0.0 action space size 67 action 42\n",
      "episode 3 step 7 reward 2.2737367544323206e-13 action space size 67 action 2\n",
      "episode 3 step 8 reward 2.2737367544323206e-13 action space size 67 action 44\n",
      "episode 3 step 9 reward 0.0 action space size 69 action 28\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "loss: 62.632064414024356\n",
      "episode 4 step 0 reward 9.245512001143652e-05 action space size 62 action 30\n",
      "episode 4 step 1 reward 0.009455087751575775 action space size 62 action 16\n",
      "episode 4 step 2 reward 0.00040392482242168626 action space size 64 action 6\n",
      "episode 4 step 3 reward 0.0006604653826798312 action space size 63 action 53\n",
      "episode 4 step 4 reward 0.10473714877525708 action space size 63 action 0\n",
      "episode 4 step 5 reward 9.094947017729282e-13 action space size 65 action 23\n",
      "episode 4 step 6 reward 4.547473508864641e-13 action space size 66 action 8\n",
      "episode 4 step 7 reward 0.0 action space size 64 action 0\n",
      "episode 4 step 8 reward 0.0 action space size 68 action 46\n",
      "episode 4 step 9 reward 0.0 action space size 68 action 67\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "loss: 12.818494841456413\n"
     ]
    }
   ],
   "source": [
    "env = make_multiple_env(**easy_config)\n",
    "N = 60\n",
    "alpha = 1e-1\n",
    "iterations = 5\n",
    "gamma = .8\n",
    "\n",
    "policy = policyNet(N+1, 128, 64)\n",
    "hidden = policy.initHidden()\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=alpha)\n",
    "policy_scheduler = torch.optim.lr_scheduler.ExponentialLR(policy_optimizer, gamma=0.1)\n",
    "rrecord = []\n",
    "for ite in range(iterations):\n",
    "    obss = []\n",
    "    acts = []\n",
    "    rews = []\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    t = 0\n",
    "    repisode = 0\n",
    "    while not d:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob, _ = policy(s, hidden)\n",
    "            prob /= prob.sum()\n",
    "\n",
    "        a = np.random.choice(s[-1].size, p = prob.numpy(), size=1)\n",
    "\n",
    "        obss.append(s)\n",
    "\n",
    "        s, r, d, _ = env.step(list(a))\n",
    "\n",
    "        print('episode', ite, 'step', t, 'reward', r, 'action space size', s[-1].size, 'action', a[0])\n",
    "\n",
    "        acts.append(a)\n",
    "        rews.append(r)\n",
    "\n",
    "        t += 1\n",
    "        repisode += r\n",
    "\n",
    "    rrecord.append(np.sum(rews))\n",
    "\n",
    "    v_hat = discounted_rewards(rews, gamma)\n",
    "    criterion = []\n",
    "\n",
    "    errs = torch.distributions.normal.Normal(0, v_hat.std())\n",
    "    for obs, act, v in zip(obss, acts, v_hat):\n",
    "        prob, hidden = policy(obs, hidden)\n",
    "        print(torch.exp(prob).sum())\n",
    "        prob_selected = prob[act]\n",
    "        hidden = hidden.detach()\n",
    "#         print(prob_selected)\n",
    "        # print(v, errs.sample(), torch.log(prob_selected))\n",
    "        loss = - (v +  errs.sample()) * prob_selected\n",
    "        \n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        policy_scheduler.step()\n",
    "        criterion.append(loss.item())\n",
    "\n",
    "    print(f'loss: {np.mean(criterion)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T08:10:45.629368Z",
     "start_time": "2021-04-15T08:10:45.625271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0416)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_hat.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T08:10:49.799179Z",
     "start_time": "2021-04-15T08:10:49.795182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1154e-02, 6.3826e-02, 6.7964e-02, 8.4450e-02, 1.0474e-01, 1.2733e-12,\n",
       "        4.5475e-13, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ieor4575]",
   "language": "python",
   "name": "conda-env-ieor4575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
