{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T21:01:49.271612Z",
     "start_time": "2021-04-16T21:01:49.106470Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T21:01:49.275432Z",
     "start_time": "2021-04-16T21:01:49.273549Z"
    }
   },
   "outputs": [],
   "source": [
    "# s = env.reset()\n",
    "# a = np.random.randint(0, s[-1].size, 1)\n",
    "# s, r, d, _ = env.step(list(a))\n",
    "# s, r, d, _ = env.step(list(a))\n",
    "# s, r, d, _ = env.step(list(a))\n",
    "# s, r, d, _ = env.step(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T21:01:49.606236Z",
     "start_time": "2021-04-16T21:01:49.277352Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Policy, self).__init__()\n",
    "        self.batchNormMatrix = nn.BatchNorm1d(num_features = num_inputs)\n",
    "        \n",
    "        self.rnn = nn.RNN(num_inputs+1, 64, 5)\n",
    "        \n",
    "        self.core = nn.Sequential(\n",
    "                                nn.Conv1d(1, 64, 3),\n",
    "                                nn.Dropout(.5),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv1d(64, 128, 3),\n",
    "                                nn.Dropout(.5),\n",
    "                                nn.Conv1d(128, 64, 3),\n",
    "                                nn.Dropout(.5),\n",
    "                                nn.Conv1d(64, 32, 3),\n",
    "                                nn.Dropout(.5),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv1d(32, 1, 3),\n",
    "                                nn.Dropout(.5),\n",
    "                                nn.ReLU(),\n",
    "                                )\n",
    "        \n",
    "#         self.rnn = nn.RNN()\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, s, hidden=None):\n",
    "        a, b, _, d, e = self._preproc(s)\n",
    "        a, d = [self.batchNormMatrix(x) for x in [a, d]]\n",
    "        b, e =[(x - x.min()) / (x.max() - x.min()) for x in [b, e]]\n",
    "        X, Y = [torch.cat((x, y.unsqueeze(1)), 1) for x, y in zip([a, d], [b, e])]\n",
    "        \n",
    "        X, Y = [x.unsqueeze(1) for x in [X, Y]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if not hidden:\n",
    "            hidden = (torch.randn(5, 1, 64), torch.randn(5, 1, 64))\n",
    "        X, X_h = self.rnn(X, hidden[0])\n",
    "        Y, Y_h = self.rnn(Y, hidden[1])\n",
    "        \n",
    "        X, Y = [F.relu(x) for x in [X, Y]]\n",
    "        \n",
    "        H, G = [self.core(x) for x in [X, Y]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        H, G = [x.squeeze(1) for x in [H, G]]\n",
    "        \n",
    "        S = H @ G.T\n",
    "        \n",
    "        \n",
    "        \n",
    "        action_scores = S.mean(0)\n",
    "        \n",
    "        \n",
    "        return F.softmax(action_scores, dim=-1), (X_h, Y_h)\n",
    "    \n",
    "    \n",
    "    def _preproc(self, s):\n",
    "        return [torch.FloatTensor(item) for item in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T21:01:49.609929Z",
     "start_time": "2021-04-16T21:01:49.607791Z"
    }
   },
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"load_dir\"        : '../instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "env = make_multiple_env(**env_config)\n",
    "\n",
    "s = env.reset()\n",
    "a = np.random.randint(0, s[-1].size, 1)\n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))\n",
    "\n",
    "\n",
    "\n",
    "model = Policy(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T21:01:49.628538Z",
     "start_time": "2021-04-16T21:01:49.611866Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class Observer(object):\n",
    "    def __init__(self, env_config):\n",
    "        self.env = make_multiple_env(**env_config)\n",
    "    def run_episode(self, agent):\n",
    "        state, ep_reward, d = self.env.reset(), 0, False\n",
    "        hidden = None\n",
    "        while not d:\n",
    "            # send the state to the agent to get an action\n",
    "            action, hidden = agent.select_action(state, hidden)\n",
    "\n",
    "            # apply the action to the environment, and get the reward\n",
    "            state, reward, d, _ = self.env.step(action)\n",
    "            # report the reward to the agent for training purpose\n",
    "            agent.report_reward(reward, d)\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, training_config,observer, model):\n",
    "        learning_rate = training_config['lr']\n",
    "        gamma = training_config['gamma']\n",
    "        \n",
    "        self.entropy_coef = training_config['entropy_coef']\n",
    "        self.observer = observer\n",
    "        self.rewards = []\n",
    "        self.gamma = gamma\n",
    "        self.policy = model\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.save_log_probs = []\n",
    "        self.save_probs = []\n",
    "        \n",
    "    def select_action(self, state, hidden):\n",
    "        probs, hidden = self.policy(state, hidden)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.save_log_probs.append(m.log_prob(action))\n",
    "        self.save_probs.append(m.probs[action])\n",
    "        return action.item(), hidden\n",
    "    def report_reward(self, reward, d):\n",
    "        if not d:\n",
    "            self.rewards.append(reward)\n",
    "        else:\n",
    "            self.rewards.append(reward)\n",
    "            self.rewards.append(np.NaN)\n",
    "        \n",
    "    def run_episode(self):\n",
    "        self.observer.run_episode(self)\n",
    "        \n",
    "    def finish_episode(self):\n",
    "        R, log_probs, probs  = 0, self.save_log_probs.copy(), self.save_probs.copy()\n",
    "        \n",
    "        rewards = []\n",
    "        rewards_seqs = []\n",
    "        rewards_seq = []\n",
    "        for reward in self.rewards:\n",
    "            if not np.isnan(reward):\n",
    "                rewards.append(reward)\n",
    "                rewards_seq.append(reward)\n",
    "            else:\n",
    "                rewards_seqs.append(rewards_seq)\n",
    "                rewards_seq = []\n",
    "        reward = min([sum(rewards_seq) for rewards_seq in rewards_seqs])\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.save_log_probs = []\n",
    "        self.save_probs = []\n",
    "        \n",
    "        policy_loss, returns = [], []\n",
    "        \n",
    "        for r in rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "        \n",
    "        for log_prob, prob, R in zip(log_probs, probs, returns):\n",
    "            policy_loss.append(-log_prob * R + self.entropy_coef * (log_prob * prob))\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T21:05:31.023301Z",
     "start_time": "2021-04-16T21:01:49.631505Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir ../instances/train_100_n60_m60 idx 0\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 1\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 2\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 3\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 4\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 5\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 6\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 7\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 8\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 9\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 10\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 11\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 12\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 13\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 14\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 15\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 16\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 17\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 18\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 19\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 20\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 21\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 22\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 23\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 24\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 25\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 26\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 27\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 28\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 29\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 30\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 31\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 32\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 33\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 34\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 35\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 36\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 37\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 38\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 39\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 40\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 41\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 42\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 43\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 44\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 45\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 46\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 47\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 48\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 49\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 50\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 51\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 52\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 53\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 54\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 55\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 56\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 57\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 58\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 59\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 60\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 61\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 62\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 63\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 64\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 65\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 66\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 67\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 68\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 69\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 70\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 71\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 72\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 73\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 74\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 75\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 76\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 77\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 78\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 79\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 80\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 81\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 82\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 83\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 84\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 85\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 86\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 87\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 88\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 89\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 90\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 91\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 92\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 93\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 94\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 95\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 96\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 97\n",
      "loading training instances, dir ../instances/train_100_n60_m60 idx 98\n",
      "Academic license - for non-commercial use only - expires 2021-06-11\n",
      "Using license file /Users/syeehyn/gurobi.lic\n",
      "iter: 0, training reward: 0.02710994429753555\n",
      "iter: 1, training reward: 1.1057218683386054\n",
      "iter: 2, training reward: 0.19109706535914484\n",
      "iter: 3, training reward: 0.14655486443825794\n",
      "iter: 4, training reward: 0.08262479645168241\n",
      "iter: 5, training reward: 0.06953034854132056\n",
      "iter: 6, training reward: 0.8301311424193045\n",
      "iter: 7, training reward: 0.6248641672173108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-6907b7c82dc9>\", line 23, in <module>\n",
      "    reward = agent.finish_episode()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/syeehyn/opt/anaconda3/envs/ieor4575/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6907b7c82dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'iter: {iteration}, training reward: {reward}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ieor4575/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "                'lr': 1e-3,\n",
    "                'gamma': .95,\n",
    "                'num_revisit': 1,\n",
    "                'entropy_coef': 1\n",
    "            }\n",
    "env_config =  {\n",
    "    \"load_dir\"        : '../instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(99)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "    }\n",
    "model = Policy(60)\n",
    "observer = Observer(env_config)\n",
    "\n",
    "\n",
    "agent = Agent(training_config, observer, model)\n",
    "\n",
    "\n",
    "for iteration in range(10):\n",
    "    for _ in range(1):\n",
    "        agent.run_episode()\n",
    "    reward = agent.finish_episode()\n",
    "    print(f'iter: {iteration}, training reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ieor4575]",
   "language": "python",
   "name": "conda-env-ieor4575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
