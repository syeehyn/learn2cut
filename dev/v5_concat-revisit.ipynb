{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:44.515519Z",
     "start_time": "2021-04-17T05:28:44.298865Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:44.853014Z",
     "start_time": "2021-04-17T05:28:44.517680Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:46.098081Z",
     "start_time": "2021-04-17T05:28:44.857778Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir ../instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 9\n",
      "Academic license - for non-commercial use only - expires 2021-06-11\n",
      "Using license file /Users/syeehyn/gurobi.lic\n"
     ]
    }
   ],
   "source": [
    "env_config = {\n",
    "    \"load_dir\"        : '../instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "env = make_multiple_env(**env_config)\n",
    "\n",
    "s = env.reset()\n",
    "a = np.random.randint(0, s[-1].size, 1)\n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:46.108062Z",
     "start_time": "2021-04-17T05:28:46.099853Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size, num_layers, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batchNormMatrix = nn.BatchNorm1d(num_features = num_inputs)\n",
    "        self.lstm = nn.LSTM(\n",
    "                    num_inputs+1,\n",
    "                    hidden_size,\n",
    "                    num_layers,\n",
    "                    bidirectional=False,\n",
    "                    dropout = dropout\n",
    "                    )\n",
    "    def forward(self, s):\n",
    "        a, b, _, d, e = self._preproc(s)\n",
    "        a, d = [self.batchNormMatrix(x) for x in [a, d]]\n",
    "        b, e =[(x - x.min()) / (x.max() - x.min()) for x in [b, e]]\n",
    "        X, Y = [torch.cat((x, y.unsqueeze(1)), 1) for x, y in zip([a, d], [b, e])]\n",
    "        X, Y = [x.unsqueeze(1) for x in [X, Y]]\n",
    "        \n",
    "        X, (X_h, X_c) = self.lstm(X)\n",
    "        Y, (Y_h, Y_c) = self.lstm(Y)\n",
    "        \n",
    "        return (X, X_h, X_c), (Y, Y_h, Y_c)\n",
    "    def _preproc(self, s):\n",
    "        return [torch.FloatTensor(item) for item in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:46.122884Z",
     "start_time": "2021-04-17T05:28:46.113077Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(torch.mul(hidden, encoder_output), dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(torch.mul(hidden, energy), dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(torch.mul(self.v, energy), dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:46.138445Z",
     "start_time": "2021-04-17T05:28:46.125632Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size, num_layers, attention, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batchNormMatrix = nn.BatchNorm1d(num_features = num_inputs)\n",
    "        self.lstm = nn.LSTM(\n",
    "                    num_inputs+1,\n",
    "                    hidden_size,\n",
    "                    num_layers,\n",
    "                    bidirectional=False,\n",
    "                    dropout = dropout\n",
    "                    )\n",
    "        self.attention = attention\n",
    "        self.cat = nn.Linear(hidden_size *2, hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, s, X_hidden, Y_hidden ,X_, Y_):\n",
    "        a, b, _, d, e = self._preproc(s)\n",
    "        a, d = [self.batchNormMatrix(x) for x in [a, d]]\n",
    "        b, e =[(x - x.min()) / (x.max() - x.min()) for x in [b, e]]\n",
    "        X, Y = [torch.cat((x, y.unsqueeze(1)), 1) for x, y in zip([a, d], [b, e])]\n",
    "        X, Y = [x.unsqueeze(1) for x in [X, Y]]\n",
    "        \n",
    "        X, (X_h, X_c) = self.lstm(X, X_hidden)\n",
    "        Y, (Y_h, Y_c) = self.lstm(Y, Y_hidden)\n",
    "        \n",
    "        \n",
    "        X_, X_h_, X_c_ = X_\n",
    "        Y_, Y_h_, Y_c_ = Y_\n",
    "        \n",
    "        X_attn_weights, Y_attn_weights = [self.attention(x, x_) for x, x_ in zip((X, Y), (X_, Y_))]\n",
    "        \n",
    "        X, Y = X.squeeze(1), Y.squeeze(1)\n",
    "        X_context = X_attn_weights.bmm(X_).squeeze(1)\n",
    "        Y_context = Y_attn_weights.bmm(Y_).squeeze(1)\n",
    "        \n",
    "        \n",
    "        X_cat_input = torch.cat((X, X_context), 1)\n",
    "        X_cat_output = F.relu(self.cat(X_cat_input))\n",
    "        \n",
    "        Y_cat_input = torch.cat((Y, Y_context), 1)\n",
    "        Y_cat_output = F.relu(self.cat(Y_cat_input))\n",
    "        \n",
    "        \n",
    "        \n",
    "        S = X_cat_output @ Y_cat_output.T\n",
    "        \n",
    "        action_scores = S.mean(0)\n",
    "        \n",
    "        \n",
    "        return F.softmax(action_scores, dim=-1), (X_h, X_c), (Y_h, Y_c)\n",
    "    \n",
    "    def _preproc(self, s):\n",
    "        return [torch.FloatTensor(item) for item in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:46.151469Z",
     "start_time": "2021-04-17T05:28:46.141016Z"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size, num_layers, method, \n",
    "                 epsilon=.8, epsilon_decay = .8, dropout=0):\n",
    "        super(Policy, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "                        num_inputs, \n",
    "                        hidden_size, \n",
    "                        num_layers, \n",
    "                        dropout\n",
    "                        )\n",
    "        attention = Attention(\n",
    "                        method,\n",
    "                        hidden_size\n",
    "                        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "                        num_inputs, \n",
    "                        hidden_size, \n",
    "                        num_layers, \n",
    "                        attention, \n",
    "                        dropout\n",
    "                        )\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        \n",
    "    def forward(self, s, hidden=None):\n",
    "        \n",
    "        output = torch.zeros(s[-1].size)\n",
    "        \n",
    "        if hidden:\n",
    "            X_hidden, Y_hidden = hidden\n",
    "        else:\n",
    "            X_hidden = torch.zeros(self.num_layers, 1, self.hidden_size), \\\n",
    "                        torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "            Y_hidden = torch.zeros(self.num_layers, 1, self.hidden_size), \\\n",
    "                        torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "        \n",
    "        X, Y = self.encoder(s)\n",
    "        \n",
    "        prob, (X_h, X_c), (Y_h, Y_c) = self.decoder(s, X_hidden, Y_hidden, X, Y)\n",
    "        \n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            prob = torch.rand(prob.shape)\n",
    "            prob /= prob.sum()\n",
    "        \n",
    "        \n",
    "        return prob, ((X_h, X_c), (Y_h, Y_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:46.179438Z",
     "start_time": "2021-04-17T05:28:46.154369Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class Observer(object):\n",
    "    def __init__(self, env_config):\n",
    "        self.env = make_multiple_env(**env_config)\n",
    "    def run_episode(self, agent):\n",
    "        state, ep_reward, d = self.env.reset(), 0, False\n",
    "        hidden = None\n",
    "        while not d:\n",
    "            # send the state to the agent to get an action\n",
    "            action, hidden = agent.select_action(state, hidden)\n",
    "\n",
    "            # apply the action to the environment, and get the reward\n",
    "            state, reward, d, _ = self.env.step(action)\n",
    "            # report the reward to the agent for training purpose\n",
    "            \n",
    "            agent.policy.epsilon *= agent.policy.epsilon_decay\n",
    "            agent.report_reward(reward, d)\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, training_config,observer, model):\n",
    "        learning_rate = training_config['lr']\n",
    "        gamma = training_config['gamma']\n",
    "        self.entropy_coef = training_config['entropy_coef']\n",
    "        self.observer = observer\n",
    "        self.rewards = []\n",
    "        self.gamma = gamma\n",
    "        self.policy = model\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "#         self.scheduler = StepLR(self.optimizer, step_size=4, gamma=0.1)\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.save_log_probs = []\n",
    "        self.save_probs = []\n",
    "        \n",
    "    def select_action(self, state, hidden):\n",
    "        probs, hidden = self.policy(state, hidden)\n",
    "        try:\n",
    "            m = Categorical(probs)\n",
    "        except ValueError:\n",
    "            probs = torch.rand(prob.shape)\n",
    "            probs /= probs.sum()\n",
    "            m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.save_log_probs.append(m.log_prob(action))\n",
    "        self.save_probs.append(m.probs[action])\n",
    "        return action.item(), hidden\n",
    "    def report_reward(self, reward, d):\n",
    "        if not d:\n",
    "            self.rewards.append(reward)\n",
    "        else:\n",
    "            self.rewards.append(reward)\n",
    "            self.rewards.append(np.NaN)\n",
    "        \n",
    "    def run_episode(self):\n",
    "        self.observer.run_episode(self)\n",
    "        \n",
    "    def finish_episode(self):\n",
    "        R, log_probs, probs  = 0, self.save_log_probs.copy(), self.save_probs.copy()\n",
    "        \n",
    "        rewards = []\n",
    "        rewards_seqs = []\n",
    "        rewards_seq = []\n",
    "        for reward in self.rewards:\n",
    "            if not np.isnan(reward):\n",
    "                rewards.append(reward)\n",
    "                rewards_seq.append(reward)\n",
    "            else:\n",
    "                rewards_seqs.append(rewards_seq)\n",
    "                rewards_seq = []\n",
    "        reward = min([sum(rewards_seq) for rewards_seq in rewards_seqs])\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.save_log_probs = []\n",
    "        self.save_probs = []\n",
    "        \n",
    "        policy_loss, returns = [], []\n",
    "        \n",
    "        for r in rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "        \n",
    "        for log_prob, prob, R in zip(log_probs, probs, returns):\n",
    "#             policy_loss.append(-log_prob * R)\n",
    "            policy_loss.append(-log_prob * R - self.entropy_coef * (log_prob * prob))\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "#         print(policy_loss)\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "#         self.scheduler.step()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T05:28:46.191309Z",
     "start_time": "2021-04-17T05:28:46.181398Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Policy(60, 128, 3, 'dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-17T05:28:44.313Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir ../instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 9\n",
      "iter: 0, training reward: 0.07657868644378141\n",
      "iter: 1, training reward: 0.05443873782223818\n",
      "iter: 2, training reward: 0.10437326798955837\n",
      "iter: 3, training reward: 0.09280565424796805\n",
      "iter: 4, training reward: 0.047775817174624535\n",
      "iter: 5, training reward: 0.1177266291279011\n",
      "iter: 6, training reward: 0.09604239726127162\n",
      "iter: 7, training reward: 0.0639455867471952\n",
      "iter: 8, training reward: 0.11308586124641806\n",
      "iter: 9, training reward: 0.1153490818633145\n",
      "iter: 10, training reward: 0.1470039690975682\n",
      "iter: 11, training reward: 0.06167352858847153\n",
      "iter: 12, training reward: 0.07742671502273879\n",
      "iter: 13, training reward: 0.044490117905297666\n",
      "iter: 14, training reward: 0.08567457563412972\n",
      "iter: 15, training reward: 0.047508427757747995\n"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "                'lr': 1e-3,\n",
    "                'gamma': .95,\n",
    "                'num_revisit': 3,\n",
    "                'entropy_coef': 1,\n",
    "            }\n",
    "env_config = {\n",
    "    \"load_dir\"        : '../instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "    }\n",
    "\n",
    "model = Policy(60, 64, 2, 'concat', dropout=.3)\n",
    "observer = Observer(env_config)\n",
    "\n",
    "\n",
    "agent = Agent(training_config, observer, model)\n",
    "\n",
    "\n",
    "for iteration in range(50):\n",
    "    for _ in range(training_config['num_revisit']):\n",
    "        agent.run_episode()\n",
    "    reward = agent.finish_episode()\n",
    "    print(f'iter: {iteration}, training reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ieor4575]",
   "language": "python",
   "name": "conda-env-ieor4575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
