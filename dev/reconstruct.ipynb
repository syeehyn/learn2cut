{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T23:43:17.660167Z",
     "start_time": "2021-04-15T23:43:17.657035Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "sys.path.append('../')\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T21:39:28.072738Z",
     "start_time": "2021-04-15T21:39:28.050578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir ../instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 9\n"
     ]
    }
   ],
   "source": [
    "easy_config = {\n",
    "    \"load_dir\"        : '../instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 10,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "env = make_multiple_env(**easy_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T00:52:20.963146Z",
     "start_time": "2021-04-16T00:52:20.960151Z"
    }
   },
   "outputs": [],
   "source": [
    "def discounted_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(0,len(r))):\n",
    "        discounted_r[i] = running_sum * gamma + r[i]\n",
    "        running_sum = discounted_r[i]\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T00:26:54.594223Z",
     "start_time": "2021-04-16T00:26:54.582939Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.batchNormMatrix = nn.BatchNorm1d(num_features = num_inputs)\n",
    "        \n",
    "        self.core = nn.Sequential(\n",
    "                                nn.Conv1d(1, 16, 3),\n",
    "                                nn.ELU(),\n",
    "                                nn.Conv1d(16, 16, 3),\n",
    "                                nn.ELU(),\n",
    "                                nn.Conv1d(16, 1, 3),\n",
    "                                nn.ELU(),\n",
    "                                )\n",
    "        \n",
    "        self.lstm = nn.LSTM(num_inputs+1 - 2 * 3, hidden_dim)\n",
    "        \n",
    "        self.critic_out = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        s, X_h, Y_h = inputs\n",
    "          \n",
    "            \n",
    "        a, b, c, d, e = self._preproc(s)\n",
    "        a, d = [self.batchNormMatrix(x) for x in [a, d]]\n",
    "        b, e =[(x - x.min()) / (x.max() - x.min()) for x in [b, e]]\n",
    "        X, Y = [torch.cat((x, y.unsqueeze(1)), 1) for x, y in zip([a, d], [b, e])]\n",
    "        \n",
    "        \n",
    "        X, Y = [x.unsqueeze(1) for x in [X, Y]]\n",
    "        \n",
    "        X, Y = [self.core(x) for x in [X, Y]]\n",
    "        \n",
    "        \n",
    "        H, G = [self.lstm(x, h) for x, h in zip([X, Y], [X_h, Y_h])]\n",
    "        \n",
    "        \n",
    "        X, Y = [x[0].squeeze(1) for x in [H, G]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_h, Y_h = [x[1] for x in [H, G]]\n",
    "        \n",
    "        \n",
    "        S = X @ Y.T\n",
    "        \n",
    "        \n",
    "        J = torch.cat((X, Y), 0)\n",
    "        \n",
    "        J = self.critic_out(J)\n",
    "        \n",
    "        \n",
    "        return S.mean(0), X_h, Y_h, J.mean()\n",
    "        \n",
    "    \n",
    "    def _preproc(self, s):\n",
    "        return [torch.FloatTensor(item) for item in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:29:04.932417Z",
     "start_time": "2021-04-16T01:29:04.920857Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    obss = []\n",
    "    rewds = []\n",
    "    acts = []\n",
    "    entropies = []\n",
    "    values = []\n",
    "    log_probs = []\n",
    "\n",
    "    s, d, repisode = env.reset(), False, 0\n",
    "\n",
    "    X_h = torch.zeros(1, 1, 32), torch.zeros(1, 1, 32)\n",
    "    Y_h = torch.zeros(1, 1, 32), torch.zeros(1, 1, 32)\n",
    "\n",
    "\n",
    "    while not d:\n",
    "        logits, X_h, Y_h, value = model((s, X_h, Y_h))\n",
    "\n",
    "        prob = F.softmax(logits, dim=-1)\n",
    "        log_prob = F.log_softmax(logits, dim=-1)\n",
    "        entropy = (-(log_prob * prob)).sum()\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        a = prob.multinomial(num_samples=1).detach()\n",
    "        log_prob = log_prob[a].sum()\n",
    "\n",
    "        a = a.numpy()\n",
    "\n",
    "        obss.append(s)\n",
    "        s, r, d, _ = env.step(list(a))\n",
    "\n",
    "        repisode += r\n",
    "        values.append(value)\n",
    "        rewds.append(r)\n",
    "        acts.append(a)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "    discounted_rewds = discounted_rewards(rewds, gamma)\n",
    "\n",
    "\n",
    "\n",
    "    value_loss = 0\n",
    "    policy_loss = 0\n",
    "    gae = 0\n",
    "    for i in range(len(rewds)):\n",
    "\n",
    "        advantages = discounted_rewds[i] - values[i]\n",
    "        value_loss += .5 * advantages.pow(2)\n",
    "\n",
    "\n",
    "        delta_t = discounted_rewds[i]\n",
    "\n",
    "        gae = gae * gamma * gae_lambda + delta_t\n",
    "\n",
    "        policy_loss -= log_probs[i] * gae - entropy_coef * entropies[i]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    (policy_loss + value_loss_coef * value_loss).backward()\n",
    "\n",
    "    print(f'value loss: {value_loss}')\n",
    "    print(f'policy loss: {policy_loss}')\n",
    "    print(f'training rewards {repisode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:00:40.010059Z",
     "start_time": "2021-04-16T01:00:40.007488Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "# THREAD = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:00:40.327180Z",
     "start_time": "2021-04-16T01:00:40.323643Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ActorCritic(N, hidden_dim)\n",
    "\n",
    "# results = multiprocess_cut(envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:00:41.578570Z",
     "start_time": "2021-04-16T01:00:40.651096Z"
    }
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "\n",
    "a = np.random.randint(0, s[-1].size, 1)   \n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))\n",
    "s, r, d, _ = env.step(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:00:41.595259Z",
     "start_time": "2021-04-16T01:00:41.581660Z"
    }
   },
   "outputs": [],
   "source": [
    "X_h = torch.zeros(1, 1, 32), torch.zeros(1, 1, 32)\n",
    "Y_h = torch.zeros(1, 1, 32), torch.zeros(1, 1, 32)\n",
    "logits, X_h, Y_h, value = model((s, X_h, Y_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:00:41.600554Z",
     "start_time": "2021-04-16T01:00:41.597912Z"
    }
   },
   "outputs": [],
   "source": [
    "prob = F.softmax(logits, dim=-1)\n",
    "log_prob = F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:00:41.605655Z",
     "start_time": "2021-04-16T01:00:41.602486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T01:30:03.463543Z",
     "start_time": "2021-04-16T01:29:07.990615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir ../instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir ../instances/train_10_n60_m60 idx 9\n",
      "iter 0\n",
      "value loss: 0.016421224921941757\n",
      "policy loss: 9.630837440490723\n",
      "training rewards 0.08013209927730713\n",
      "iter 1\n",
      "value loss: 0.007653901819139719\n",
      "policy loss: 5.1265106201171875\n",
      "training rewards 0.043967092729417345\n",
      "iter 2\n",
      "value loss: 0.014788379892706871\n",
      "policy loss: 8.923904418945312\n",
      "training rewards 0.05890729972497866\n",
      "iter 3\n",
      "value loss: 0.04046788066625595\n",
      "policy loss: 17.518695831298828\n",
      "training rewards 0.1105047281384941\n",
      "iter 4\n",
      "value loss: 0.015446520410478115\n",
      "policy loss: 9.380786895751953\n",
      "training rewards 0.06911933128867531\n",
      "iter 5\n",
      "value loss: 0.04602259024977684\n",
      "policy loss: 19.609460830688477\n",
      "training rewards 0.13023474748570152\n",
      "iter 6\n",
      "value loss: 0.018577054142951965\n",
      "policy loss: 10.494180679321289\n",
      "training rewards 0.10043944296512564\n",
      "iter 7\n",
      "value loss: 0.024954158812761307\n",
      "policy loss: 13.016797065734863\n",
      "training rewards 0.07887106790803955\n",
      "iter 8\n",
      "value loss: 0.004407873377203941\n",
      "policy loss: 2.5223891735076904\n",
      "training rewards 0.016204641118292784\n",
      "iter 9\n",
      "value loss: 0.0576903373003006\n",
      "policy loss: 22.245582580566406\n",
      "training rewards 0.14196066778254135\n",
      "iter 10\n",
      "value loss: 0.02797030098736286\n",
      "policy loss: 13.907251358032227\n",
      "training rewards 0.1012208294150696\n",
      "iter 11\n",
      "value loss: 0.010267716832458973\n",
      "policy loss: 6.683770179748535\n",
      "training rewards 0.05264089070101363\n",
      "iter 12\n",
      "value loss: 0.010796316899359226\n",
      "policy loss: 6.939716815948486\n",
      "training rewards 0.05082072332606913\n",
      "iter 13\n",
      "value loss: 1.2324085235595703\n",
      "policy loss: 113.39810943603516\n",
      "training rewards 0.7082653196252977\n",
      "iter 14\n",
      "value loss: 0.007194723468273878\n",
      "policy loss: 4.711543560028076\n",
      "training rewards 0.05096988495097321\n",
      "iter 15\n",
      "value loss: 0.0052927215583622456\n",
      "policy loss: 3.252429246902466\n",
      "training rewards 0.01747595509959865\n",
      "iter 16\n",
      "value loss: 0.025723889470100403\n",
      "policy loss: 12.848356246948242\n",
      "training rewards 0.13678196182490865\n",
      "iter 17\n",
      "value loss: 0.025252288207411766\n",
      "policy loss: 12.790085792541504\n",
      "training rewards 0.06628136492645353\n",
      "iter 18\n",
      "value loss: 0.08661998063325882\n",
      "policy loss: 28.151966094970703\n",
      "training rewards 0.22612779265682548\n",
      "iter 19\n",
      "value loss: 0.004447193816304207\n",
      "policy loss: 2.5753092765808105\n",
      "training rewards 0.017372672572037118\n"
     ]
    }
   ],
   "source": [
    "gae_lambda = 1\n",
    "entropy_coef = .01\n",
    "value_loss_coef = .5\n",
    "\n",
    "N = 60\n",
    "gamma = .99\n",
    "hidden_dim = 32\n",
    "\n",
    "num_iteration = 20\n",
    "\n",
    "alpha = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=alpha)\n",
    "\n",
    "\n",
    "env = make_multiple_env(**easy_config)\n",
    "model = ActorCritic(N, hidden_dim)\n",
    "\n",
    "for it in range(num_iteration):\n",
    "    print(f'iter {it}')\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T00:53:18.629541Z",
     "start_time": "2021-04-16T00:53:18.621754Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-543-efb6c09e9517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_optim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSharedAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSharedAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "from my_optim import SharedAdam\n",
    "optimizer = SharedAdam(model.parameters(), lr=args.lr)\n",
    "optimizer.share_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ieor4575]",
   "language": "python",
   "name": "conda-env-ieor4575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
